{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009313cf-cf16-4463-9047-f86e279b4702",
   "metadata": {},
   "source": [
    "# L1: LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e9e951-18af-4b59-9a28-46a22705ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (1.101.0)\n",
      "Requirement already satisfied: dotenv in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.9.9)\n",
      "Requirement already satisfied: tiktoken in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.11.0)\n",
      "Requirement already satisfied: utils in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: panel in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (1.4.5)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: sniffio in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.62.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2025.7.34)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from tiktoken) (2.26.0)\n",
      "Requirement already satisfied: markdown in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (3.8.2)\n",
      "Requirement already satisfied: markdown-it-py in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (3.0.0)\n",
      "Requirement already satisfied: pyviz-comms>=2.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (3.0.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (2.3.2)\n",
      "Requirement already satisfied: bleach in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (4.0.0)\n",
      "Requirement already satisfied: bokeh<3.5.0,>=3.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (3.4.3)\n",
      "Requirement already satisfied: param<3.0,>=2.1.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (2.2.1)\n",
      "Requirement already satisfied: mdit-py-plugins in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (0.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (2025.4.0)\n",
      "Requirement already satisfied: linkify-it-py in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from panel) (2.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.2)\n",
      "Requirement already satisfied: contourpy>=1.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (1.3.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (6.5.2)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (2.11.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (8.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (6.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (25.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bokeh<3.5.0,>=3.4.0->panel) (2.0.2)\n",
      "Requirement already satisfied: certifi in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.9->bokeh<3.5.0,>=3.4.0->panel) (1.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2->panel) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2->panel) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.2->panel) (2.9.0.post0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->panel) (1.17.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: webencodings in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from bleach->panel) (0.5.1)\n",
      "Requirement already satisfied: uc-micro-py in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from linkify-it-py->panel) (1.0.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from markdown->panel) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown->panel) (3.6.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py->panel) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openai dotenv tiktoken utils panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d646f6-acec-4583-90f2-38a4bcff350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d2ab0d-d313-4037-8ef5-6577e3b6c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da850d-1a5c-4ac4-98f6-7c132fb6fef6",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb3674b-67cd-4880-915f-69370d292359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9021e42-927b-4e34-8b23-2655f41b269d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c62ddfe-1791-4035-b9ef-ef1c3cf8abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174fe02e-4bad-4b07-ac48-15c1b65ef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96743e10-a2ee-42a0-b996-f25a6ef9bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175039dc-6e74-4b1e-98c5-da323b638a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba538f2-33d7-4526-ab88-bd85eba76acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ah, I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2a756-617a-40d2-976c-a0db970de087",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ab51e2-16a6-4300-bb9b-f11509b34899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.3.31)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.4.16)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.4.22)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.26.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (1.101.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: idna in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: anyio in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2021.10.8)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.62.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.7.34)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0f707-f590-4488-8fd7-fd81d1069c45",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b945dd3c-934e-4b76-a7fb-4a90a75f4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "458c65bc-21f6-4ab4-a537-5690ea896239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f89f2000940>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f89f20136d0>, root_client=<openai.OpenAI object at 0x7f89f2000490>, root_async_client=<openai.AsyncOpenAI object at 0x7f89e16d4a90>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20b802-e09d-4be9-bb80-6a4220537a8e",
   "metadata": {},
   "source": [
    "### Prompt template\n",
    "\n",
    "#### Prompt template provides abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce52cf7c-b66c-4c46-a512-7d20469efc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e03c99b-0406-4f1c-b5e6-43e4a23964e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8662327-9044-4c5a-9907-3226217bfbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8b7ff4-b17e-4955-bea2-2949ae3a5666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8852f7-7104-45f8-8ed0-3326e7178207",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb0cb1eb-d471-4fd3-858b-e7ee6451a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a295e42-60fd-45f6-8a2e-2c0220a5ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0256732c-bb9a-4801-82b8-a8cb96dd2b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82fd968b-3875-4d2b-a50c-9df00eec96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "573d1bf9-f172-465e-99c8-f818ee473edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a01c54-0332-4459-aaeb-8d09a4298a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh man, I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And on top of that, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, buddy.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb3f5291-8313-4f1d-9b89-32229c17cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42fab34a-7844-45ca-9ab5-070def3ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80aa89b1-2278-4dfb-beff-b5570c25241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "067371af-75c0-46c8-8843-50cc9336adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/r4mvcwds07d3q0n_m0y_vm7m0000gn/T/ipykernel_3271/92171669.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  service_response = chat(service_messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, valued customer! Regrettably, the warranty be not coverin' the costs o' cleanin' yer galley due to yer own negligence. Ye see, 'twas yer own doin' when ye forgot to secure the lid afore startin' the blender. 'Tis a tough break, indeed! Fare thee well, matey!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1408a12-2baf-42fd-b00a-1005f3fc4432",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f08ee6a0-712a-4ba9-97de-c7645ddfd9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw Python dictionary output\n",
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc92119a-9514-4341-842a-60b17481bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "\n",
    "present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31648203-ffcc-4bc9-9051-6e76da428f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c9d1880-6522-4770-a898-f73ca912c764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddf2c3e0-5575-4e6d-8822-c9c0cad5a0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c90da79-b884-4adb-8ce1-ae606e1c47f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sf/r4mvcwds07d3q0n_m0y_vm7m0000gn/T/ipykernel_3271/912055596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# because'gift' is not a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 'gift' is a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code \n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae092a35-4070-4837-8144-d9b10eaf4f3f",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c25c8e-10d6-49b7-8cfc-40b4226f2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1bbe9a4-bb75-40ed-8662-7002db997ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2fda118-bee6-48e8-883b-a8b1692e5111",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f01b9009-64c4-4a67-b424-2b56ab66997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1df69b91-8bbb-411d-881b-5844b55e1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95e3d7e2-5529-449e-b737-cea6583892c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ab7ce11a-1bc3-4e47-8376-9db5dafb02eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's \n",
      "present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "284c572b-fb70-41a8-91a3-e8a5d01e85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f6956d8-1de1-4548-a77c-087329683216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d53bcc05-9bec-4d12-9178-e2378066965f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "395d4b5e-f00e-40d2-b0d8-931dea388e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c059c9a0-faf0-4fd0-95b1-22cf7afab3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5149f-5057-4555-9619-e2b4b4941f43",
   "metadata": {},
   "source": [
    "# L2: LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory\n",
    "\n",
    "## Additional Types\n",
    "* Vector data memory - Stores text in a vector db and retirves the most relevant blocks of text\n",
    "* Entity memory - Using an LLM, it remembers details about specific entities.\n",
    "\n",
    "## Notes:\n",
    "* We can use multiple memories at one time. E.g., Conversation memory + Entity memory to recall individuals.\n",
    "* We can also store the conversations in a conventional database (such as key-value store or sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c89caf-c52b-4ca5-a82e-76577fc11304",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9d52e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-openai in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-community in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (0.3.27)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/langchain-community/\u001b[0m\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.4.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (2.26.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (1.4.22)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-openai) (1.101.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from aiosignal>=1.4.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: anyio in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
      "Requirement already satisfied: certifi in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (2021.10.8)\n",
      "Requirement already satisfied: idna in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (3.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.62.3)\n",
      "Requirement already satisfied: sniffio in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.7.34)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sonalpdas/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade langchain langchain-openai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "013fa930-9398-4b0d-90c6-4ce47e6fda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7fc23ae-4f9b-4566-baa1-82aa4eba4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False # Set True to see Assistant's monologue\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "504357ec-3b5b-446c-8e07-799c512f283f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! That's great to hear. LangChain is a fantastic course that covers a wide range of topics related to language learning and communication. Are you enjoying the course so far?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, I am learnning LangChain course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa0b07a8-b3e6-4fd7-9998-1f387c03776c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1+1 equals 2. It's a basic arithmetic operation that involves adding two numbers together. Is there anything else you'd like to know or discuss?\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "301bf2a0-a6af-41c0-b5d8-897e034c6a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are learning the LangChain course, which focuses on language learning and communication. It covers various topics related to different languages, language acquisition, and effective communication strategies. It's a great course to enhance your language skills and understanding. Is there anything specific you'd like to know about the course?\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What course am I learning?\") # To check if LLM remember's context / prior conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8775a2b3-589a-406f-b45b-4bfcd459bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, I am learnning LangChain course\n",
      "AI: Hello! That's great to hear. LangChain is a fantastic course that covers a wide range of topics related to language learning and communication. Are you enjoying the course so far?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. It's a basic arithmetic operation that involves adding two numbers together. Is there anything else you'd like to know or discuss?\n",
      "Human: What course am I learning?\n",
      "AI: You are learning the LangChain course, which focuses on language learning and communication. It covers various topics related to different languages, language acquisition, and effective communication strategies. It's a great course to enhance your language skills and understanding. Is there anything specific you'd like to know about the course?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) # List all context/conversations in the buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21df8f2c-dedc-4306-8b95-9152e69713e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2887878359.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/sf/r4mvcwds07d3q0n_m0y_vm7m0000gn/T/ipykernel_3271/2887878359.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    memory.load_memory_variables({}) // # {} is empty dictionary, LangChain provides options to load memory\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({}) // # {} is empty dictionary, LangChain provides options to load memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8fa5f2a-75d6-4119-ac64-40aaa23e8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory() # Create new memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad1d8fc6-a3dd-40c1-a5fd-adcfcf3c6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0b311d5-5f80-49ac-8bd4-20f456e4fa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ca265d5-7449-49d0-883c-11ea0c4ad6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef422926-5074-41a1-856a-04408f62e62e",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4581d-ac58-4103-89cd-5924a2af7ea9",
   "metadata": {},
   "source": [
    "#### LLM are stateless\n",
    "#### LangChain provides several kinds of memory to store and accumulate the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c92e0c9c-964e-4441-9fbf-495db748a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "951da0b0-d12d-4709-93ec-4545ce26aca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1) # k=1 means remember one conversation (from LLM and user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42622f41-4a13-4a4c-aa06-f24111400b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecd96f8f-443a-4af9-99d0-798a929c4678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab67cfab-b90b-4a5a-8247-39792b55b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1) # Memory keeps latest k conversations (from both LLM and user)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0810595d-ff42-450b-ab58-096608eeee15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, I am learnning LangChain course\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! That's great to hear. LangChain is a fantastic course that covers a wide range of topics related to language learning and communication. Are you enjoying the course so far?\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, I am learnning LangChain course\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fec2c622-b5c8-41d2-bbd6-f66da6a32418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, I am learnning LangChain course\n",
      "AI: Hello! That's great to hear. LangChain is a fantastic course that covers a wide range of topics related to language learning and communication. Are you enjoying the course so far?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1+1 equals 2. It's a basic arithmetic operation that involves adding two numbers together. Is there anything else you'd like to know or discuss?\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39db9038-a726-4764-ac60-61e47f86d59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. It's a basic arithmetic operation that involves adding two numbers together. Is there anything else you'd like to know or discuss?\n",
      "Human: What course am I learning?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to that information. Can you provide me with more context so I can assist you better?\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What course am I learning?\") # To check if LLM remember's context / prior conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5072ef0-3cbc-499f-b280-c4b512d2eda2",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory\n",
    "\n",
    "#### Limits the tokens saved in the memory, since most models charge based on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7735893c-c3c1-4255-86d7-9c3aa8e9e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5d73093e-7560-48a9-a474-e5e1a88e25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae373bd2-0ad0-4d29-816e-aa0a61d59b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e216bd-b4b7-4a3b-a110-d2279aa72944",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "\n",
    "#### Instead of limiting the tokens, the conversation is summarized to the token limit by LLM and save it in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e369145b-dd7d-467c-b9d7-0970b4276bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ff1af57-31b0-461f-90e6-37b5b229b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "52099028-fd15-40dd-b51a-cf6b40e865c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\"}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b18166e3-23ce-49d1-a773-c3bbd4aadeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ") # This conversation is using Memory of type ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09895fcc-65a0-4b70-8a9e-f69d1d18b6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the morning meeting with the product team, a demo showcasing the latest features and updates on the LangChain project would be ideal. This could include a live demonstration of the language processing capabilities, the integration of new algorithms, and any improvements in performance metrics. Additionally, for the lunch meeting with the customer interested in AI, a demo highlighting the practical applications of AI in their specific industry would be impactful. This could involve showcasing case studies, success stories, and potential use cases tailored to their needs.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e6d7e3e-9a6e-40ab-8e46-f65fd287c90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI exchange greetings and discuss the schedule for the day. The AI informs the human of a morning meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the importance of being prepared for the day's events, suggesting demos for the morning meeting showcasing the latest features of LangChain and for the lunch meeting tailored to the customer's industry needs.\"}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d083830-b347-430b-8094-974525061664",
   "metadata": {},
   "source": [
    "# L3: Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c6a82cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade --force-reinstall numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00dc7a43-b3b0-4188-8ecc-ebb018a53193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b907bb",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bbc78d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33b9a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3bd6ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76f0b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "52d39936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regal Rest Bedding Co.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11bb91",
   "metadata": {},
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06263e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "73126f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5825403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86edfd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "83d59427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRegal Linens\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRegal Linens offers luxurious bedding, bath linens, and home décor items to elevate your living space with style and comfort.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regal Linens offers luxurious bedding, bath linens, and home décor items to elevate your living space with style and comfort.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b9fac8",
   "metadata": {},
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b993e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f682454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "05fbeb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4924d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2abcd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5ccc1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bdd629b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste poor. The foam doesn't hold, it's weird. I buy the same ones in the store and the taste is much better... Old batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is disappointed with the poor taste and lack of foam in the product, leading them to question if it is possibly an old batch or counterfeit.',\n",
       " 'followup_message': \"Cher client,\\n\\nNous sommes désolés d'apprendre que vous avez été déçu par la saveur médiocre et le manque de mousse de notre produit. Nous tenons à vous assurer que nous mettons tout en œuvre pour garantir la qualité de nos produits et nous prenons vos commentaires très au sérieux. Il est possible qu'il s'agisse d'un lot ancien ou contrefait, et nous aimerions enquêter plus en détail pour résoudre ce problème. Nous vous remercions de nous avoir informés et nous vous prions de bien vouloir nous contacter pour que nous puissions trouver une solution satisfaisante.\\n\\nCordialement,\\nL'équipe du service client\"}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7be208",
   "metadata": {},
   "source": [
    "## Router Chain\n",
    "\n",
    "#### Routes an input to a specific change relevant to it.\n",
    "#### Input ->LLM-> Subjects (Router Chain) -> Destination chain  (Sub chain) if input belongs to one of the subjects\n",
    "####                                          -> Default chain if input does not belong to any subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f45f2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "18864811",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73239663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7850d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7f71bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69c315a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bddff849",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60045c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fb693d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e74cff49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation is the electromagnetic radiation emitted by a perfect absorber and emitter of radiation, known as a black body. A black body absorbs all radiation that falls on it and emits radiation at all wavelengths. The spectrum of black body radiation is continuous and depends only on the temperature of the black body. This phenomenon is described by Planck's law, which states that the intensity of radiation emitted by a black body at a given wavelength is proportional to the temperature of the body and the wavelength raised to the fifth power.\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33c8b722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1acaa135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "biology: Good for answering biology questions: {'input': 'Why does every cell in our body contain DNA?'}"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received invalid destination chain name 'biology: Good for answering biology questions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sf/r4mvcwds07d3q0n_m0y_vm7m0000gn/T/ipykernel_3271/785140960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Why does every cell in our body contain DNA?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    628\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    408\u001b[0m         }\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    411\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             outputs = (\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/router/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Received invalid destination chain name '{route.destination}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     async def _acall(\n",
      "\u001b[0;31mValueError\u001b[0m: Received invalid destination chain name 'biology: Good for answering biology questions'"
     ]
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc279022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
